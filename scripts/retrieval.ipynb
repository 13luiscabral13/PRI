{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_NUMBER = \"q4_2\"\n",
    "QUERY_PATH = '../queries/' + QUERY_NUMBER + '/' + QUERY_NUMBER\n",
    "\n",
    "BASE_QUERY_URL = \"http://localhost:8983/solr/games/select?debugQuery=false&fl=*%2C%20%5Bchild%5D&fq=%7B!child%20of%3D%22*%3A*%20-_nest_path_%3A*%22%7Dname%3A(family%20switch)%20OR%20summary%3A(family%20switch)%20OR%20wikipedia%3A(family%20switch)%20OR%20genre%3A(family%20switch)&indent=true&q.op=OR&q=review%3A(family%20switch)%0Aplatform%3A(family%20switch)&rows=5000&useParams=&wt=json\"\n",
    "BOOSTED_QUERY_URL = \"http://localhost:8983/solr/games/select?bq=%7B!child%20of%3D%22*%3A*%20-_nest_path_%3A*%22%7Dname%3A(family%20switch)%5E3%20OR%20summary%3A(family%20switch)%20OR%20wikipedia%3A(family%20switch)%5E2%20OR%20genre%3A(family%20switch)%5E6&debugQuery=false&defType=edismax&fl=*%2C%20%5Bchild%5D&fq=%7B!child%20of%3D%22*%3A*%20-_nest_path_%3A*%22%7Dname%3A*&indent=true&pf=2&ps=3&q.op=OR&q=family%20switch&qf=review%5E2%20platform%5E11&rows=5000&useParams=&wt=json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executa a base query e guarda os documentos obtidos no ficheiro baseRank.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query retorna reviews\n",
    "base_results_reviews = requests.get(BASE_QUERY_URL).json()['response']['docs']\n",
    "base_gameids = []\n",
    "base_results = []\n",
    "\n",
    "for index, doc in enumerate(base_results_reviews):\n",
    "    if len(base_gameids) == 30:\n",
    "        break\n",
    "    \n",
    "    if doc['id'].split('/')[0] not in base_gameids:\n",
    "        #print(\"\\nGetting game with id: \", doc['id'].split('/')[0])\n",
    "        base_gameids.append(doc['id'].split('/')[0])\n",
    "        query_url = \"http://localhost:8983/solr/games/select?fl=*%2C%5Bchild%5D&indent=true&q.op=OR&q=id%3A(\" + doc['id'].split('/')[0] + ')&useParams=&wt=json'\n",
    "        result = requests.get(query_url).json()['response']['docs']\n",
    "        #print(result)\n",
    "        base_results.append(result[0])\n",
    "\n",
    "base_ranked_doc = [doc['name'] for index, doc in enumerate(base_results)]\n",
    "\n",
    "df_base = pd.DataFrame(base_ranked_doc, columns=['BASE'], index=None)\n",
    "\n",
    "print(df_base)\n",
    "\n",
    "latex_table = df_base.to_latex(index=False)\n",
    "\n",
    "with open(QUERY_PATH+'_baseRank.txt', 'w') as tf:\n",
    "    tf.write(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query retorna reviews\n",
    "boosted_results_reviews = requests.get(BOOSTED_QUERY_URL).json()['response']['docs']\n",
    "boosted_gameids = []\n",
    "boosted_results = []\n",
    "\n",
    "for index, doc in enumerate(boosted_results_reviews):\n",
    "    if len(boosted_gameids) == 30:\n",
    "        break\n",
    "    \n",
    "    if doc['id'].split('/')[0] not in boosted_gameids:\n",
    "        #print(\"\\nGetting game with id: \", doc['id'].split('/')[0])\n",
    "        boosted_gameids.append(doc['id'].split('/')[0])\n",
    "        query_url = \"http://localhost:8983/solr/games/select?fl=*%2C%5Bchild%5D&indent=true&q.op=OR&q=id%3A(\" + doc['id'].split('/')[0] + ')&useParams=&wt=json'\n",
    "        result = requests.get(query_url).json()['response']['docs']\n",
    "        #print(result)\n",
    "        boosted_results.append(result[0])\n",
    "\n",
    "boosted_ranked_doc = [doc['name'] for index, doc in enumerate(boosted_results)]\n",
    "\n",
    "df_boosted = pd.DataFrame(boosted_ranked_doc, columns=['BOOSTED'], index=None)\n",
    "\n",
    "print(df_boosted)\n",
    "\n",
    "latex_table = df_boosted.to_latex(index=False)\n",
    "\n",
    "with open(QUERY_PATH+'_boostedRank.txt', 'w') as tf:\n",
    "    tf.write(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depois construir o ficheiro com todos os documentos relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = list(map(lambda el: el.strip(), open(QUERY_PATH+'_relevant.txt').readlines()))\n",
    "\n",
    "print(relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição das métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "metric = lambda f: metrics.setdefault(f.__name__, f)\n",
    "\n",
    "@metric\n",
    "def p30(results, relevant, n=30):\n",
    "    \"\"\"Precision at N\"\"\"\n",
    "    return len([doc for doc in results[:n] if doc['name'] in relevant])/n\n",
    "\n",
    "@metric\n",
    "def ap(results, relevant):\n",
    "    \"\"\"Average Precision\"\"\"\n",
    "    precision_values = []\n",
    "    relevant_count = 0\n",
    "    for idx, doc in enumerate(results):\n",
    "        if doc['name'] in relevant:\n",
    "            relevant_count += 1\n",
    "            precision_at_k = relevant_count / (idx + 1)\n",
    "            precision_values.append(precision_at_k)\n",
    "    #print(precision_values)\n",
    "    if not precision_values:\n",
    "        return 0.0\n",
    "\n",
    "    return sum(precision_values)/len(precision_values)\n",
    "\n",
    "def calculate_metric(key, results, relevant):\n",
    "    return metrics[key](results, relevant)\n",
    "\n",
    "evaluation_metrics = {\n",
    "    'ap': 'Average Precision',\n",
    "    'p30': 'Precision at 30 (P@30)'\n",
    "}\n",
    "\n",
    "#print(\"Boosted Results: \", boosted_results)\n",
    "#print(\"Base Results: \", base_results)\n",
    "\n",
    "# Calcular e imprimir as métricas\n",
    "for metric_key, metric_name in evaluation_metrics.items():\n",
    "    metric_value_base = calculate_metric(metric_key, base_results, relevant)\n",
    "    metric_value_boosted = calculate_metric(metric_key, boosted_results, relevant)\n",
    "    print(f'{metric_name}: {metric_value_base}   {metric_value_boosted}')\n",
    "    \n",
    "# Calcular métricas\n",
    "metrics_results_base = [calculate_metric(metric_key, base_results, relevant) for metric_key in evaluation_metrics.keys()]\n",
    "metrics_results_boosted = [calculate_metric(metric_key, boosted_results, relevant) for metric_key in evaluation_metrics.keys()]\n",
    "\n",
    "# Criar DataFrame do Pandas para os resultados das métricas\n",
    "metrics_df = pd.DataFrame(list(zip(evaluation_metrics.values(), metrics_results_base, metrics_results_boosted)), columns=['Metrics', 'Base System', 'Boosted System'])\n",
    "\n",
    "# Converter o DataFrame para uma tabela LaTeX sem um índice\n",
    "latex_table_metrics = metrics_df.to_latex(index=False)\n",
    "\n",
    "# Escrever a tabela LaTeX em um arquivo\n",
    "with open(QUERY_PATH + \"_metrics.tex\", 'w') as tf:\n",
    "    tf.write(latex_table_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função que cálcula a curva de Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(results):\n",
    "    precision_values = [\n",
    "        round(\n",
    "        len([\n",
    "            doc \n",
    "            for doc in results[:idx]\n",
    "            if doc['name'] in relevant\n",
    "        ]) / idx, 2 )\n",
    "        for idx, _ in enumerate(results, start=1)\n",
    "    ]\n",
    "    \n",
    "    #print(\"Precision list: \", precision_values)\n",
    "    \n",
    "    recall_values = [\n",
    "        round(\n",
    "        len([\n",
    "            doc for doc in results[:idx]\n",
    "            if doc['name'] in relevant\n",
    "        ]) / len(relevant), 2)\n",
    "        for idx, _ in enumerate(results, start=1)\n",
    "    ]\n",
    "    \n",
    "    #print(\"Recall list: \", recall_values)\n",
    "    \n",
    "    precision_recall_match = {k: v for k,v in zip(recall_values, precision_values)}\n",
    "    \n",
    "    # Extend recall_values to include traditional steps for a better curve (0.1, 0.2 ...)\n",
    "    recall_values.extend([step for step in np.arange(0.1, 1.1, 0.1) if step not in recall_values])\n",
    "    recall_values = sorted(set(recall_values))\n",
    "\n",
    "    # Extend matching dict to include these new intermediate steps\n",
    "    for idx, step in enumerate(recall_values):\n",
    "        if step not in precision_recall_match:\n",
    "            if recall_values[idx-1] in precision_recall_match:\n",
    "                precision_recall_match[step] = precision_recall_match[recall_values[idx-1]]\n",
    "            else:\n",
    "                precision_recall_match[step] = precision_recall_match[recall_values[idx+1]]\n",
    "    \n",
    "        \n",
    "    #print(\"Precision Recall Match:\")\n",
    "    #print(precision_recall_match)            \n",
    "    \n",
    "    tolerance = 0.0001  # adjust the tolerance as needed\n",
    "\n",
    "    filtered_data = {round(key,1): value for key, value in precision_recall_match.items() if any(abs(key - allowed_key) < tolerance for allowed_key in np.arange(0.1, 1.1, 0.1))}\n",
    "    print(\"\\nFiltered\")\n",
    "    print(filtered_data)\n",
    "    \n",
    "    return precision_recall_match, recall_values\n",
    "\n",
    "pr_match_base, recall_values_base = evaluate(base_results)\n",
    "pr_match_boosted, recall_values_boosted = evaluate(boosted_results)\n",
    "\n",
    "# Plotar um único gráfico para ambos os sistemas\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_values_base, [pr_match_base.get(r) for r in recall_values_base], label='Base System')\n",
    "plt.plot(recall_values_boosted, [pr_match_boosted.get(r) for r in recall_values_boosted], label='Boosted System')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(QUERY_PATH + '_combined_pr.pdf')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_document_ids = [doc['name'] for doc in base_results]\n",
    "base_relevance_column = ['Y' if doc_id in relevant else 'N' for doc_id in base_document_ids]\n",
    "\n",
    "boosted_document_ids = [doc['name'] for doc in boosted_results]\n",
    "boosted_relevance_column = ['Y' if doc_id in relevant else 'N' for doc_id in boosted_document_ids]\n",
    "\n",
    "ranked_documents = list(zip(range(1, len(base_results) + 1), base_document_ids, base_relevance_column, boosted_document_ids, boosted_relevance_column))\n",
    "\n",
    "df1 = pd.DataFrame(ranked_documents, columns=['Rank', 'Base System', 'Relevance', 'Boosted System', 'Relevance'])\n",
    "\n",
    "print(df1)\n",
    "# Convert the DataFrame to a LaTeX table without an index\n",
    "latex_table = df1.to_latex(index=False)\n",
    "\n",
    "# Write the LaTeX table to a file\n",
    "with open(QUERY_PATH+'_ranked_documents.tex', 'w') as tf:\n",
    "    tf.write(latex_table)\n",
    "\n",
    "# Print the LaTeX table\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
